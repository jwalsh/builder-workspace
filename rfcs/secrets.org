#+TITLE: RFC: Ollama Integration Options for Project Manager
#+AUTHOR: AI Assistant
#+DATE: [2024-03-19 Tue]

* Introduction

This RFC proposes options for integrating Ollama with our Project Manager application, focusing on efficient use of resources and flexibility for different environments.

* Options

** Option 1: Run Ollama in Docker Container

This option involves running Ollama within the Docker environment, alongside our Project Manager.

*** Docker Compose Configuration

#+BEGIN_SRC yaml :tangle docker-compose.yml
version: '3'
services:
  project_manager:
    build: .
    volumes:
      - .:/app
      - ./secrets.json:/app/secrets.json
    depends_on:
      - ollama
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434

  ollama:
    image: ollama/ollama
    volumes:
      - ./ollama_data:/root/.ollama
#+END_SRC

*** Pros
- Contained within Docker environment
- Consistent setup across different machines

*** Cons
- Requires additional resources within Docker
- May duplicate Ollama instance if already running on host

** Option 2: Forward Ollama Port from Host

This option involves using the Ollama instance running on the host machine.

*** Docker Compose Configuration

#+BEGIN_SRC yaml :tangle docker-compose.host-ollama.yml
version: '3'
services:
  project_manager:
    build: .
    volumes:
      - .:/app
      - ./secrets.json:/app/secrets.json
    environment:
      - OLLAMA_API_BASE_URL=http://host.docker.internal:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
#+END_SRC

*** Pros
- Uses existing Ollama instance on host
- Saves resources by not duplicating Ollama

*** Cons
- Requires Ollama to be running on host
- May need additional configuration for non-Docker Desktop environments

* Secrets Management

To manage secrets across different environments, we'll use a `secrets.json` file for local development and Docker, with provisions for other environments.

** Local Development and Docker

Create a `secrets.json` file:

#+BEGIN_SRC json :tangle secrets-example.json
{
  "GOOGLE_AI_API_KEY": "your_google_ai_api_key_here",
  "ANTHROPIC_API_KEY": "your_anthropic_api_key_here",
  "GITHUB_TOKEN": "your_github_token_here",
  "OLLAMA_API_BASE_URL": "http://localhost:11434"
}
#+END_SRC

Mount this file in Docker Compose:

#+BEGIN_SRC yaml
volumes:
  - ./secrets.json:/app/secrets.json
#+END_SRC

** Other Environments (e.g., Replit, AWS)

For environments like Replit or AWS, we'll need to adapt our secrets management:

*** Replit
Use Replit's built-in Secrets management. Update our code to check for environment variables first:

#+BEGIN_SRC python :tangle project_manager.py
import os
import json

def load_secrets():
    if os.path.exists('secrets.json'):
        with open('secrets.json') as f:
            return json.load(f)
    else:
        return {
            "GOOGLE_AI_API_KEY": os.getenv("GOOGLE_AI_API_KEY"),
            "ANTHROPIC_API_KEY": os.getenv("ANTHROPIC_API_KEY"),
            "GITHUB_TOKEN": os.getenv("GITHUB_TOKEN"),
            "OLLAMA_API_BASE_URL": os.getenv("OLLAMA_API_BASE_URL", "http://localhost:11434")
        }

secrets = load_secrets()
#+END_SRC

*** AWS
For AWS, we can use AWS Secrets Manager with a boto3 script. Here's a sample implementation:

#+BEGIN_SRC python :tangle aws_secrets.py
import boto3
import json
from botocore.exceptions import ClientError

def get_aws_secret(secret_name, region_name="us-west-2"):
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region_name)

    try:
        get_secret_value_response = client.get_secret_value(SecretId=secret_name)
    except ClientError as e:
        raise e
    else:
        if 'SecretString' in get_secret_value_response:
            return json.loads(get_secret_value_response['SecretString'])

# Usage
aws_secrets = get_aws_secret("your_secret_name")
#+END_SRC

* Implementation

1. Create both Docker Compose files (`docker-compose.yml` and `docker-compose.host-ollama.yml`).
2. Update `project_manager.py` to use the `load_secrets()` function.
3. Create `secrets-example.json` and add `secrets.json` to `.gitignore`.
4. For AWS deployments, include `aws_secrets.py` and update `project_manager.py` to use it when in an AWS environment.

* Recommendation

For local development and most use cases, Option 2 (forwarding Ollama port from host) is recommended. It allows for efficient resource usage and leverages the existing Ollama instance on the host machine.

For production or cloud deployments, Option 1 (running Ollama in Docker) provides a more consistent and isolated environment.

* Next Steps

1. Implement the chosen option in the project.
2. Update documentation to reflect the chosen setup.
3. Test the setup in various environments (local, Replit, AWS) to ensure compatibility.
4. Create a contributing guide that explains how to set up the development environment, including Ollama integration.
