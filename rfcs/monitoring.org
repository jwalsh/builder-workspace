#+TITLE: RFC: Ollama Integration and Monitoring Options for Project Manager
#+AUTHOR: AI Assistant
#+DATE: [2024-03-19 Tue]

* Introduction

This RFC proposes options for integrating Ollama with our Project Manager application and implementing monitoring solutions. It considers various deployment scenarios, including running directly on a laptop, within containers, or on cloud/bare metal systems.

* Deployment Options

** Option 1: Run Directly on Laptop

This option involves running the Project Manager and Ollama directly on the host machine.

*** Setup

1. Install Python and create a virtual environment
2. Install Ollama on the host machine
3. Run the Project Manager script

#+BEGIN_SRC bash :tangle setup_laptop.sh
#!/bin/bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
curl https://ollama.ai/install.sh | sh
python project_manager.py
#+END_SRC

*** Pros
- Simplest setup
- Direct access to host resources and file system
- Easy to monitor and debug

*** Cons
- Less isolated environment
- Potential conflicts with other Python projects
- Manual setup required for each new machine

** Option 2: Run in Docker Container

This option involves running both the Project Manager and Ollama within Docker containers.

*** Docker Compose Configuration

#+BEGIN_SRC yaml :tangle docker-compose.yml
version: '3'
services:
  project_manager:
    build: .
    volumes:
      - .:/app
      - ./secrets.json:/app/secrets.json
    depends_on:
      - ollama
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434

  ollama:
    image: ollama/ollama
    volumes:
      - ./ollama_data:/root/.ollama

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  node_exporter:
    image: quay.io/prometheus/node-exporter
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  grafana_data:
#+END_SRC

*** Pros
- Consistent environment across different machines
- Easy to scale and deploy
- Includes monitoring setup

*** Cons
- Requires Docker knowledge
- Slightly more resource-intensive

** Option 3: Run on Cloud/Bare Metal with Monitoring

This option involves deploying the Project Manager and Ollama on a cloud or bare metal server with monitoring.

*** Setup Script

#+BEGIN_SRC bash :tangle setup_server.sh
#!/bin/bash

# Install dependencies
sudo apt update
sudo apt install -y python3 python3-venv python3-pip prometheus node_exporter grafana

# Set up Python environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Install Ollama
curl https://ollama.ai/install.sh | sh

# Start monitoring services
sudo systemctl start prometheus
sudo systemctl start node_exporter
sudo systemctl start grafana-server

# Run Project Manager
python project_manager.py
#+END_SRC

*** Pros
- Full control over the environment
- Suitable for production deployments
- Includes robust monitoring

*** Cons
- Requires server management skills
- More complex setup

* Monitoring Setup

For all options, we'll implement monitoring using Prometheus, Node Exporter, and Grafana.

** Prometheus Configuration

#+BEGIN_SRC yaml :tangle prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']

  - job_name: 'project_manager'
    static_configs:
      - targets: ['localhost:8000']  # Assuming Project Manager exposes metrics on port 8000
#+END_SRC

** Grafana Dashboard

Create a Grafana dashboard to visualize:
- System metrics (CPU, memory, disk usage)
- Project Manager specific metrics (task count, processing time, etc.)
- Ollama performance metrics

* Secrets Management

We'll use a flexible approach to secrets management that works across different environments.

#+BEGIN_SRC python :tangle project_manager.py
import os
import json

def load_secrets():
    if os.path.exists('secrets.json'):
        with open('secrets.json') as f:
            return json.load(f)
    else:
        return {
            "GOOGLE_AI_API_KEY": os.getenv("GOOGLE_AI_API_KEY"),
            "ANTHROPIC_API_KEY": os.getenv("ANTHROPIC_API_KEY"),
            "GITHUB_TOKEN": os.getenv("GITHUB_TOKEN"),
            "OLLAMA_API_BASE_URL": os.getenv("OLLAMA_API_BASE_URL", "http://localhost:11434")
        }

secrets = load_secrets()
#+END_SRC

* Implementation Steps

1. Create a `setup_laptop.sh` script for local development.
2. Create `docker-compose.yml` for containerized deployment.
3. Create `setup_server.sh` for cloud/bare metal deployment.
4. Update `project_manager.py` to include the `load_secrets()` function.
5. Create `prometheus.yml` for monitoring configuration.
6. Implement metrics collection in `project_manager.py`:

#+BEGIN_SRC python :tangle project_manager.py
from prometheus_client import start_http_server, Counter, Gauge

# Metrics
task_counter = Counter('project_manager_tasks_total', 'Total number of tasks processed')
processing_time = Gauge('project_manager_processing_time_seconds', 'Time taken to process a task')

def main():
    start_http_server(8000)  # Start metrics server
    # ... rest of the main function
#+END_SRC

7. Create a Grafana dashboard for visualizing metrics.

* Recommendation

For development and personal use, Option 1 (running directly on the laptop) provides the simplest setup and easiest access to local project files and environments.

For team development or light production use, Option 2 (Docker containers) offers a good balance of consistency and ease of deployment.

For full production deployment, Option 3 (cloud/bare metal with monitoring) provides the most control and scalability.

Regardless of the chosen option, implementing the monitoring setup is crucial for maintaining visibility into the application's performance and health.

* Next Steps

1. Implement the chosen option in the project.
2. Set up the monitoring stack and create Grafana dashboards.
3. Update the project documentation to reflect the chosen setup and monitoring instructions.
4. Conduct thorough testing in the target environment.
5. Create a runbook for common operational tasks and troubleshooting.
