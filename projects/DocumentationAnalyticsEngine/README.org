* Documentation Analytics Engine

This project provides analytics on documentation usage, helping to identify areas for improvement and expansion.

** Project Structure

The project is organized as follows:

#+BEGIN_SRC
doc-analytics-engine/
├── config/
│   ├── project_info.json
│   └── tasks.json
├── data/
│   ├── us-v-google-2023/
│   │   ├── DTX0365.pdf
│   │   ├── DTX0498_0.pdf
│   │   ├── DTX0712_Replacement.pdf
│   │   ...
│   │   └── PTX1857_Redacted_0.pdf
│   └── mirrors/
│       └── [DATE_TIME]/
│           └── us-and-plaintiff-states-v-google-llc-2023-trial-exhibits/
│               ├── DTX0365.pdf
│               ├── DTX0498_0.pdf
│               ...
│               └── PTX1857_Redacted_0.pdf
├── deployment/
│   └── deploy_and_monitor.sh
├── design/
│   ├── analytics_engine.md
│   ├── data_pipeline.md
│   ├── system_architecture.md
│   └── user_interface.md
├── docs/
│   └── write_documentation.md
├── infrastructure/
│   ├── modules/
│   │   ├── s3/
│   │   ├── lambda/
│   │   ├── dynamodb/
│   │   ├── api_gateway/
│   │   └── bedrock/
│   ├── dev/
│   │   └── main.tf
│   └── prod/
│       └── main.tf
├── rfcs/
│   └── rfc_1882_project_scope.md
├── scripts/
│   ├── build_project.sh
│   ├── count_lines.sh
│   ├── mirror_justice_gov.sh
│   └── update_repo.sh
├── src/
│   ├── analytics/
│   ├── lambda_functions/
│   ├── api/
│   └── utils/
├── tests/
│   └── test_and_validate.py
└── README.org
#+END_SRC

** System Architecture

The system architecture is represented in the following diagram:

#+BEGIN_SRC mermaid
graph TD
    Users((Users)) -->|1. Upload PDFs| s3:raw[S3: Raw PDFs]
    s3:raw -->|2. Trigger| lambda:process[Lambda: Process PDFs]
    lambda:process -->|3. Extract text| textract:service[Amazon Textract]
    textract:service -->|4. Extracted text| s3:extracted[S3: Extracted Text]
    s3:extracted -->|5. Trigger| lambda:analyze[Lambda: Analyze Text]
    lambda:analyze -->|6. Analyze| bedrock:service[Amazon Bedrock]
    bedrock:service -->|7. Analysis results| s3:analyzed[S3: Analyzed Data]
    
    lambda:process -->|8. Store metadata| dynamodb:metadata[DynamoDB: Metadata]
    lambda:analyze -->|9. Store insights| dynamodb:metadata
    
    Users -->|10. Query| apigateway:get[API Gateway: GET API]
    apigateway:get -->|11. Handle request| lambda:query[Lambda: Query Function]
    lambda:query -->|12. Search| opensearch:service[Amazon OpenSearch]
    opensearch:service -->|13. Search| s3:analyzed
    opensearch:service -->|14. Retrieve metadata| dynamodb:metadata
    
    subgraph AWS Cloud
        s3:raw
        lambda:process
        textract:service
        s3:extracted
        lambda:analyze
        bedrock:service
        s3:analyzed
        dynamodb:metadata
        apigateway:get
        lambda:query
        opensearch:service
    end
    
    classDef aws fill:#FF9900,stroke:#232F3E,stroke-width:2px,color:#232F3E;
    class s3:raw,lambda:process,textract:service,s3:extracted,lambda:analyze,bedrock:service,s3:analyzed,dynamodb:metadata,apigateway:get,lambda:query,opensearch:service aws;
#+END_SRC

This diagram illustrates the flow of data and processing in our Documentation Analytics Engine, utilizing various AWS services including Amazon Bedrock for advanced analysis and Amazon OpenSearch for efficient querying.

** Infrastructure

We use Terraform to manage our infrastructure. The configuration is split into dev and prod environments, located in the `infrastructure/` directory.

*** Development Environment (us-west-2)

To set up the development environment:

#+BEGIN_SRC shell
cd infrastructure/dev
terraform init
terraform plan
terraform apply
#+END_SRC

*** Production Environment (us-east-2)

To set up the production environment:

#+BEGIN_SRC shell
cd infrastructure/prod
terraform init
terraform plan
terraform apply
#+END_SRC

** Data Import

The trial exhibits were initially imported using the following command:

#+BEGIN_SRC shell
mv us-and-plaintiff-states-v-google-llc-2023-trial-exhibits/* data/us-v-google-2023
#+END_SRC

** Mirroring Justice.gov

To create an up-to-date mirror of the trial exhibits from the Justice.gov website, use the =mirror_justice_gov.sh= script in the =scripts= directory. This script will create a dated archive of the exhibits.

To run the script:

#+BEGIN_SRC shell
./scripts/mirror_justice_gov.sh
#+END_SRC

The mirrored data will be saved in the =data/mirrors/[DATE_TIME]/= directory, organized by date and time. Each mirror will contain the specific path =us-and-plaintiff-states-v-google-llc-2023-trial-exhibits/= with the relevant PDF files.

** Usage

1. Set up the project structure by running =./scripts/build_project.sh=
2. Import or mirror the required data as described above
3. Deploy the infrastructure using Terraform for the desired environment
4. Implement the analytics engine in the =src/analytics= directory
5. Deploy Lambda functions and API Gateway endpoints
6. Run tests using =./tests/test_and_validate.py=
7. Monitor the system using AWS CloudWatch and the custom =deploy_and_monitor.sh= script

** License

MIT License

Copyright (c) 2023 Documentation Analytics Engine Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

** Contributing

Contributions to the Documentation Analytics Engine are welcome. Please follow these steps to contribute:

1. Fork the repository
2. Create a new branch for your feature or bug fix
3. Make your changes and commit them with clear, descriptive messages
4. Push your changes to your fork
5. Submit a pull request to the main repository

Please ensure your code adheres to the existing style and passes all tests before submitting a pull request.
